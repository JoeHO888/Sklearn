from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_predict
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pylot  as plot

le=preprocessing.LabelEncoder() 
le.fit(data[0:2,:])

#Originally want to use one hot encoding, but it increase the computational cost greatly and I want to make the category more general
#Alternative assign similar features for a category of features similar value and different features a huge amount of difference
#ie sleeping and reading similar value: 1 and 1.1 and 
X=np.hstack((le.transform(data[:,:2]),preprocessing.scale(data[:,2:]))
y=data[5]

	
#use sklearn.datasets.make_regression to generate data
	
get.request("https://localhost:5000/learn")	
	


lr=LinearRegression()
lr.fit(X,y)
joblib.dump(lr,"lr_machine.pkl")
lr=joblib.load(lr,"lr_machine.pkl")

print(lr.predict(something))


check overfitting:
train_sizes, train_loss, test_loss = learning_curve(LinearRegression(), X, y, cv=10, scoring='neg_mean_squared_error',train_sizes=np.linspace((0,1,10)))
train_loss_mean = -np.mean(train_loss, axis=1)
test_loss_mean = -np.mean(test_loss, axis=1)
plt.plot(train_sizes, train_loss_mean, 'o-', color="r",
         label="Training")
plt.plot(train_sizes, test_loss_mean, 'o-', color="g",
        label="Cross-validation")

plt.xlabel("Training examples")
plt.ylabel("Loss")
plt.legend(loc="best")
plt.show()
	
